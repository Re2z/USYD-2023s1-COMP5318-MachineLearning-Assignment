{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group number: A1part2 100  , SID1: 520638064 , SID2:   520026168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the specific dataset\n",
    "dataset = pd.read_csv('breast-cancer-wisconsin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the dataset\n",
    "\n",
    "# Replace the missing value \"?\" to NA, and replace the classes 'class1' and 'class2' to 0 and 1\n",
    "dataset.replace(['?', 'class1', 'class2'], [np.nan, 0, 1], inplace=True)\n",
    "\n",
    "# Using the sklearn.impute.SimpleImputer to replace the missing value to the mean value of the column\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "dataset = imp_mean.fit_transform(dataset)\n",
    "\n",
    "# Normalise the features values between [0,1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# Separate the dataset to the features and class, the last column is the class\n",
    "X_data = dataset[:, :-1]\n",
    "y_data = dataset[:, -1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444,0.0000,0.0000,0.0000,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.4444,0.3333,0.3333,0.4444,0.6667,1.0000,0.2222,0.1111,0.0000,0\n",
      "0.2222,0.0000,0.0000,0.0000,0.1111,0.1111,0.2222,0.0000,0.0000,0\n",
      "0.5556,0.7778,0.7778,0.0000,0.2222,0.3333,0.2222,0.6667,0.0000,0\n",
      "0.3333,0.0000,0.0000,0.2222,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.7778,1.0000,1.0000,0.7778,0.6667,1.0000,0.8889,0.6667,0.0000,1\n",
      "0.0000,0.0000,0.0000,0.0000,0.1111,1.0000,0.2222,0.0000,0.0000,0\n",
      "0.1111,0.0000,0.1111,0.0000,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.1111,0.0000,0.0000,0.0000,0.1111,0.0000,0.0000,0.0000,0.4444,0\n",
      "0.3333,0.1111,0.0000,0.0000,0.1111,0.0000,0.1111,0.0000,0.0000,0\n"
     ]
    }
   ],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "            \n",
    "# Print the first 10 rows of the pre-processed dataset\n",
    "print_data(X_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "    \"\"\"\n",
    "    Takes a numpy data array and target and return the average accuracy\n",
    "    Achieve the Logistic Regression Classifier\n",
    "\n",
    "    :param X: numpy array of shape (n_examples, n_features)\n",
    "    :param y: numpy array of shape (n_examples)\n",
    "    :return: the average accuracy of classifier\n",
    "    \"\"\"\n",
    "    # Initialize the Logistic Regression classifier\n",
    "    logreg = LogisticRegression(random_state=0)\n",
    "    # Calculate the cross-validation score\n",
    "    scores = cross_val_score(logreg, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "    \"\"\"\n",
    "    Takes a numpy data array and target and return the average accuracy\n",
    "    Achieve the Naïve Bayes Classifier\n",
    "\n",
    "    :param X: numpy array of shape (n_examples, n_features)\n",
    "    :param y: numpy array of shape (n_examples)\n",
    "    :return: the average accuracy of classifier\n",
    "    \"\"\"\n",
    "    # Initialize the Naïve Bayes classifier\n",
    "    nb = GaussianNB()\n",
    "    # Calculate the cross-validation score\n",
    "    scores = cross_val_score(nb, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "    \"\"\"\n",
    "    Takes a numpy data array and target and return the average accuracy\n",
    "    Achieve the Decision Tree Classifier\n",
    "\n",
    "    :param X: numpy array of shape (n_examples, n_features)\n",
    "    :param y: numpy array of shape (n_examples)\n",
    "    :return: the average accuracy of classifier\n",
    "    \"\"\"\n",
    "    # Initialize the Decision Tree classifier\n",
    "    dt = DecisionTreeClassifier(random_state=0, criterion=\"entropy\")\n",
    "    # Calculate the cross-validation score\n",
    "    scores = cross_val_score(dt, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    \"\"\"\n",
    "    Takes a numpy data array and target and return the average accuracy\n",
    "    Achieve the Bagging Classifier\n",
    "\n",
    "    :param X: numpy array of shape (n_examples, n_features)\n",
    "    :param y: numpy array of shape (n_examples)\n",
    "    :param n_estimators: number of base estimators in the ensemble\n",
    "    :param max_samples: number of samples to draw from X to train each base estimator\n",
    "    :param max_depth: maximum depth of the tree\n",
    "    :return: the average accuracy of classifier\n",
    "    \"\"\"\n",
    "    # Initialize the Bagging classifier using the Decision Tree classifier\n",
    "    dt = DecisionTreeClassifier(random_state=0, criterion=\"entropy\", max_depth=max_depth)\n",
    "    bag = BaggingClassifier(dt, n_estimators=n_estimators, max_samples=max_samples, random_state=0)\n",
    "    # Calculate the cross-validation score\n",
    "    scores = cross_val_score(bag, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    \"\"\"\n",
    "    Takes a numpy data array and target and return the average accuracy\n",
    "    Achieve the AdaBoost Classifier\n",
    "\n",
    "    :param X: numpy array of shape (n_examples, n_features)\n",
    "    :param y: numpy array of shape (n_examples)\n",
    "    :param n_estimators: maximum number of estimators at which boosting is terminated\n",
    "    :param learning_rate: weight applied to each classifier at each boosting iteration\n",
    "    :param max_depth: maximum depth of the tree\n",
    "    :return: the average accuracy of classifier\n",
    "    \"\"\"\n",
    "    # Initialize the AdaBoost classifier using the Decision Tree classifier\n",
    "    dt = DecisionTreeClassifier(random_state=0, criterion=\"entropy\", max_depth=max_depth)\n",
    "    ada = AdaBoostClassifier(dt, learning_rate=learning_rate, n_estimators=n_estimators, random_state=0)\n",
    "    # Calculate the cross-validation score\n",
    "    scores = cross_val_score(ada, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    \"\"\"\n",
    "    Takes a numpy data array and target and return the average accuracy\n",
    "    Achieve the Gradient Boosting Classifier\n",
    "\n",
    "    :param X: numpy array of shape (n_examples, n_features)\n",
    "    :param y: numpy array of shape (n_examples)\n",
    "    :param n_estimators: number of boosting stages to perform\n",
    "    :param learning_rate: weight applied to each classifier at each boosting iteration\n",
    "    :return: the average accuracy of classifier\n",
    "    \"\"\"\n",
    "    # Initialize the Gradient Boosting classifier\n",
    "    gb = GradientBoostingClassifier(learning_rate=learning_rate, n_estimators=n_estimators, random_state=0)\n",
    "    # Calculate the cross-validation score\n",
    "    scores = cross_val_score(gb, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR average cross-validation accuracy: 0.9642\n",
      "NB average cross-validation accuracy: 0.9585\n",
      "DT average cross-validation accuracy: 0.9385\n",
      "Bagging average cross-validation accuracy: 0.9571\n",
      "AdaBoost average cross-validation accuracy: 0.9599\n",
      "GB average cross-validation accuracy: 0.9613\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 60\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 6\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 60\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 6\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 60\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: %.4f\" % logregClassifier(X_data, y_data))\n",
    "print(\"NB average cross-validation accuracy: %.4f\" % nbClassifier(X_data, y_data))\n",
    "print(\"DT average cross-validation accuracy: %.4f\" % dtClassifier(X_data, y_data))\n",
    "print(\"Bagging average cross-validation accuracy: %.4f\" % bagDTClassifier(X_data, y_data, bag_n_estimators, bag_max_samples, bag_max_depth))\n",
    "print(\"AdaBoost average cross-validation accuracy: %.4f\" % adaDTClassifier(X_data, y_data, ada_n_estimators, ada_learning_rate, ada_bag_max_depth))\n",
    "print(\"GB average cross-validation accuracy: %.4f\" % gbClassifier(X_data, y_data, gb_n_estimators, gb_learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, stratify=y_data, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7, 9]\n",
    "p = [1, 2]\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    \"\"\"\n",
    "    Takes a numpy data array and target and return the average accuracy\n",
    "    Achieve the Grid Search KNN Classifier\n",
    "\n",
    "    :param X: numpy array of shape (n_examples, n_features)\n",
    "    :param y: numpy array of shape (n_examples)\n",
    "    :return: the best parameter, cross-validation, test accuracy of the best model\n",
    "    \"\"\"\n",
    "    # Set the parameter grid for grid search\n",
    "    param_grid = {'n_neighbors': k, 'p': p}\n",
    "    # Initialize and train the grid search classifier for KNN\n",
    "    knn = GridSearchCV(KNeighborsClassifier(), param_grid, cv=cvKFold, return_train_score=True)\n",
    "    knn.fit(X, y)\n",
    "    # Obtain the parameters and accuracy for the best model\n",
    "    best_k = knn.best_params_['n_neighbors']\n",
    "    best_p = knn.best_params_['p']\n",
    "    cv_accuracy = knn.best_score_\n",
    "    test_accuracy = knn.score(X_test, y_test)\n",
    "    return best_k, best_p, cv_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "C = [0.01, 0.1, 1, 5, 15] \n",
    "gamma = [0.01, 0.1, 1, 10, 50]\n",
    "\n",
    "def bestSVMClassifier(X, y):\n",
    "    \"\"\"\n",
    "    Takes a numpy data array and target and return the average accuracy\n",
    "    Achieve the Grid Search SVM Classifier\n",
    "\n",
    "    :param X: numpy array of shape (n_examples, n_features)\n",
    "    :param y: numpy array of shape (n_examples)\n",
    "    :return: the best parameter, cross-validation, test accuracy of the best model\n",
    "    \"\"\"\n",
    "    # Set the parameter grid for grid search\n",
    "    param_grid = {'C': C, 'gamma': gamma}\n",
    "    # Initialize and train the grid search classifier for SVM\n",
    "    svm = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=cvKFold, return_train_score=True)\n",
    "    svm.fit(X, y)\n",
    "    # Obtain the parameters and accuracy for the best model\n",
    "    best_C = svm.best_params_['C']\n",
    "    best_gamma = svm.best_params_['gamma']\n",
    "    cv_accuracy = svm.best_score_\n",
    "    test_accuracy = svm.score(X_test, y_test)\n",
    "    return best_C, best_gamma, cv_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 30, 60, 100, 150]\n",
    "max_leaf_nodes = [6, 12, 18]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    \"\"\"\n",
    "    Takes a numpy data array and target and return the average accuracy\n",
    "    Achieve the Grid Search Random Forest Classifier\n",
    "    \n",
    "    :param X: numpy array of shape (n_examples, n_features)\n",
    "    :param y: numpy array of shape (n_examples)\n",
    "    :return: the best parameter, cross-validation, test accuracy, F1-score of the best model\n",
    "    \"\"\"\n",
    "    # Set the parameter grid for grid search\n",
    "    param_grid = {'n_estimators': n_estimators, 'max_leaf_nodes': max_leaf_nodes}\n",
    "    # Initialize and train the grid search classifier for RandomForest\n",
    "    rf = GridSearchCV(RandomForestClassifier(criterion=\"entropy\", max_features=\"sqrt\", random_state=0), param_grid, cv=cvKFold, return_train_score=True)\n",
    "    rf.fit(X, y)\n",
    "    # Predict the results of the test data\n",
    "    _pred = rf.predict(X_test)\n",
    "    # Obtain the parameters and accuracy for the best model\n",
    "    best_n = rf.best_params_['n_estimators']\n",
    "    best_leaf = rf.best_params_['max_leaf_nodes']\n",
    "    cv_accuracy = rf.best_score_\n",
    "    test_accuracy = rf.score(X_test, y_test)\n",
    "    # Calculate the F1-score by predict label and target label\n",
    "    f1_macro = f1_score(y_test, _pred, average='macro')\n",
    "    f1_weight = f1_score(y_test, _pred, average='weighted')\n",
    "    return best_n, best_leaf, cv_accuracy, test_accuracy, f1_macro, f1_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 3\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.9695\n",
      "KNN test set accuracy: 0.9543\n",
      "\n",
      "SVM best C: 5.0000\n",
      "SVM best gamma: 0.1000\n",
      "SVM cross-validation accuracy: 0.9676\n",
      "SVM test set accuracy: 0.9714\n",
      "\n",
      "RF best n_estimators: 150\n",
      "RF best max_leaf_nodes: 6\n",
      "RF cross-validation accuracy: 0.9675\n",
      "RF test set accuracy: 0.9657\n",
      "RF test set macro average F1: 0.9628\n",
      "RF test set weighted average F1: 0.9661\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "print(\"KNN best k: {}\".format(bestKNNClassifier(X_train, y_train)[0]))\n",
    "print(\"KNN best p: {}\".format(bestKNNClassifier(X_train, y_train)[1]))\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(bestKNNClassifier(X_train, y_train)[2]))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(bestKNNClassifier(X_train, y_train)[3]))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"SVM best C: {:.4f}\".format(bestSVMClassifier(X_train, y_train)[0]))\n",
    "print(\"SVM best gamma: {:.4f}\".format(bestSVMClassifier(X_train, y_train)[1]))\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\".format(bestSVMClassifier(X_train, y_train)[2]))\n",
    "print(\"SVM test set accuracy: {:.4f}\".format(bestSVMClassifier(X_train, y_train)[3]))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"RF best n_estimators: {}\".format(bestRFClassifier(X_train, y_train)[0]))\n",
    "print(\"RF best max_leaf_nodes: {}\".format(bestRFClassifier(X_train, y_train)[1]))\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(bestRFClassifier(X_train, y_train)[2]))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(bestRFClassifier(X_train, y_train)[3]))\n",
    "print(\"RF test set macro average F1: {:.4f}\".format(bestRFClassifier(X_train, y_train)[4]))\n",
    "print(\"RF test set weighted average F1: {:.4f}\".format(bestRFClassifier(X_train, y_train)[5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
