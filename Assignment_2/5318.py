# -*- coding: utf-8 -*-
"""5318.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o-4vmw-WiM9j9TwELgrI7mRtig87ng4S

# COMP5318 assignment 2: Classification

### Group number: 100 , SID1: 520026168 , SID2:  , SID3:

## Setup and Dependencies
"""

! pip install scikeras
! pip install seaborn
import pickle
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras
from sklearn.decomposition import PCA
from scikeras.wrappers import KerasClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import recall_score, precision_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings("ignore")

"""## 1. Data Loading"""

#File integrity verification and visualization

# Load the training and testing data
train_file = 'EMNIST_Byclass_Small/emnist_train.pkl'
test_file = 'EMNIST_Byclass_Small/emnist_test.pkl'

with open(train_file, 'rb') as f:
    train_data = pickle.load(f, encoding='bytes')

with open(test_file, 'rb') as f:
    test_data = pickle.load(f, encoding='bytes')

# Print the keys of the dictionaries
print(train_data.keys())
print(test_data.keys())

# Extract the data and labels from the dictionaries
X_train_img, y_train = train_data['data'], train_data['labels']
X_test_img, y_test = test_data['data'], test_data['labels']

# Flatten the images and convert the pixel values to floats
X_train = X_train_img.reshape(X_train_img.shape[0], -1).astype(np.float32)
X_test = X_test_img.reshape(X_test_img.shape[0], -1).astype(np.float32)
X_train_img = X_train_img.astype(np.float32)
X_test_img = X_test_img.astype(np.float32)

# Print the shapes of the data and labels
print('X_train shape:', X_train.shape)
print('X_train_img shape:', X_train_img.shape)
print('X_test shape:', X_test.shape)
print('X_test_img shape:', X_test_img.shape)
print('y_train shape:', y_train.shape)
print('y_test shape:', y_test.shape)

# Visualize a sample image from the dataset
plt.imshow(X_train_img[0], cmap='gray')
plt.title('Label: ' + str(y_train[0]))
plt.show()

"""## 2. Data Preprocessing

This section includes the data visualization of the label distributions.

As well as principal component analysis and Normalization.

### 2.1 Data Visualization and Descriptive Analysis

Plot the distribution of labels and display some sample images.
"""

# Create a DataFrame to store the labels and their counts
labels_df = pd.DataFrame({'label': y_train})
labels_df['count'] = 1

# Plot the distribution of labels
sns.set_style(style='whitegrid')
plt.figure(figsize=(12, 6))
sns.countplot(x='label', data=labels_df)
plt.title('Distribution of Labels')
plt.show()

# Display some sample images
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(X_train_img[i], cmap=plt.cm.binary)
    plt.xlabel(y_train[i])
plt.show()

# Calculate the mean and standard deviation of the pixel values
pixel_mean = np.mean(X_train_img)
pixel_std = np.std(X_train_img)
print('Mean of pixel values:', pixel_mean)
print('Standard deviation of pixel values:', pixel_std)

"""### 2.2 One-hot Coding"""

# Change the label to the one-hot mode
y_train_one = tf.keras.utils.to_categorical(y_train, 62)
y_test_one = tf.keras.utils.to_categorical(y_test, 62)

print('y_train_one shape:', y_train_one.shape)
print('y_test_one shape:', y_test_one.shape)

"""### 2.3 Normalization"""

# Normalize the pixel values to be between 0 and 1
X_train_norm = X_train / 255.0
X_test_norm = X_test / 255.0
X_train_img_norm = X_train_img / 255.0
X_test_img_norm = X_test_img / 255.0

# Print the shapes of the preprocessed data
print('X_train_norm shape:', X_train_norm.shape)
print('X_test_norm shape:', X_test_norm.shape)
print('X_train_img_norm shape:', X_train_img_norm.shape)
print('X_test_img_norm shape:', X_test_img_norm.shape)

"""### 2.4 PCA

Select the best PCA components.
"""

# Perform PCA without reducing dimensionality pca = PCA()
pca = PCA()
pca.fit(X_train_norm)
cumsum = np.cumsum(pca.explained_variance_ratio_)

# Plot the explained variance vs number of dimensions
plt.figure(figsize=(12, 6))
plt.plot(cumsum, linewidth=3)
plt.axis([0, 784, 0.6, 1])
plt.xlabel("dimensions")
plt.ylabel("explained variance")
plt.grid(True)
plt.show()

"""Select 0.95 as the components and apply."""

# Apply PCA to reduce the dimensionality of the data
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_train_norm)
X_test_pca = pca.transform(X_test_norm)

# Scale the data to have zero mean and unit variance
scaler = StandardScaler()
X_train_pca = scaler.fit_transform(X_train_pca)
X_test_pca = scaler.transform(X_test_pca)

# Print the shapes of the preprocessed data
print('X_train_pca shape:', X_train_pca.shape)
print('X_test_pca shape:', X_test_pca.shape)

"""## 3. Three Models

### 3.1 K-Nearest Neighbour

Calculate the metrics.
"""

# KNN classifier and train
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X=X_train_pca, y=y_train)

# Predict the test data
accuracy_knn = knn.score(X=X_test_pca, y=y_test)
pred_knn = knn.predict(X=X_test_pca)

# Calculate and print the metrics
precision_knn = precision_score(y_true=y_test, y_pred=pred_knn, average='macro')
recall_knn = recall_score(y_true=y_test, y_pred=pred_knn, average='macro')
cm_knn = confusion_matrix(y_true=y_test, y_pred=pred_knn)

print("Accuracy on test set: {:.3f}%".format(100 * accuracy_knn))
print("Precision on test set: {:.3f}%".format(100 * precision_knn))
print("Recall on test set: {:.3f}%".format(100 * recall_knn))
print("Confusion_matrix: \n", cm_knn)

"""Classification report."""

# Classification report
print(classification_report(y_true=y_test, y_pred=pred_knn))

"""Visualize the confusion matrix."""

# Visualize the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(data=cm_knn, fmt="d", cmap="RdBu_r", center=500)
plt.title("knn Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

"""### 3.2 Fully Connected Neural Network"""

# Ensure stability across runs
keras.backend.clear_session()
tf.random.set_seed(42)

"""MLP model"""

# Define MLP model by layer
model_NN = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[X_train_norm.shape[1]]),
    keras.layers.Dense(units=500, activation='tanh'),  # relu
    keras.layers.Dense(units=300, activation='tanh'),  # relu
    keras.layers.Dense(units=62, activation='softmax')  # output layer
])

model_NN.summary()

"""Compile and train."""

# Instantiate optimizer and compile the model
model_NN.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),
                 metrics=['accuracy', 'Precision', 'Recall'])

# Train the classifier
model_NN_results = model_NN.fit(x=X_train_norm, y=y_train_one, epochs=10, batch_size=64, shuffle=True,
                                validation_split=0.2)

"""Visualize the training results."""

# Convert the history result dictionary to a Pandas dataframe and extract the accuracies
loss_NN = pd.DataFrame(model_NN_results.history)[['loss', 'val_loss']]
accuracy_NN = pd.DataFrame(model_NN_results.history)[['accuracy', 'val_accuracy']]
precision_NN = pd.DataFrame(model_NN_results.history)[['precision', 'val_precision']]
recall_NN = pd.DataFrame(model_NN_results.history)[['recall', 'val_recall']]

# Plot
loss_NN.plot()
plt.xlabel('Epoch')
plt.show()

accuracy_NN.plot()
plt.xlabel('Epoch')
plt.show()

precision_NN.plot()
plt.xlabel('Epoch')
plt.show()

recall_NN.plot()
plt.xlabel('Epoch')
plt.show()

"""Calculate the metrics on test set."""

# Evaluate the test data
loss_NN_test, accuracy_NN_test, precision_NN_test, recall_NN_test = model_NN.evaluate(x=X_test_norm, y=y_test_one)

pred_NN = model_NN.predict(x=X_test_norm)
cm_NN = confusion_matrix(y_true=y_test, y_pred=np.argmax(pred_NN, axis=1))

print("Loss on test set: {:.4f}".format(loss_NN_test))
print("Accuracy on test set: {:.4f}%".format(100 * accuracy_NN_test))
print("Precision on test set: {:.4f}%".format(100 * precision_NN_test))
print("Recall on test set: {:.4f}%".format(100 * recall_NN_test))
print("Confusion_matrix: \n", cm_NN)

"""Classification report"""

print(classification_report(y_true=y_test, y_pred=pred_NN))

"""Visualize the confusion matrix."""

plt.figure(figsize=(8, 6))
sns.heatmap(data=cm_NN, fmt="d", cmap="RdBu_r", center=500)
plt.title("NN Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

"""### 3.3 ResNet-18

ResNet-18 model
"""

# The residual block
def residual_block(inputs, out_channel, activation_fn, same_shape=False):
    """
    The residual block in ResNet-18.

    :param inputs: input tensor to the residual block
    :param out_channel: number of output channels/filters for the convolutional layers in the block
    :param activation_fn: activation function to be applied after each convolutional layer
    :param same_shape: a boolean flag indicating whether the input and output tensors should have the same shape
                       If False, the input tensor will be downsampled by a factor of 2
    :return: output tensor of the residual block
    """
    strides = (2, 2) if same_shape else (1, 1)

    # Convolution layers
    x = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=(3, 3), strides=strides, padding="same")(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(activation=activation_fn)(x)
    x = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=(3, 3), padding="same")(x)
    x = tf.keras.layers.BatchNormalization()(x)

    # Downsample the input if needed
    if same_shape:
        inputs = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=(1, 1), strides=(2, 2), padding="same")(inputs)

    # Add the residual connection
    x = tf.keras.layers.Add()([x, inputs])
    x = tf.keras.layers.Activation(activation=activation_fn)(x)

    return x


# ResNet 18
def ResNet(activation_fn):
    """
    ResNet-18 Model.

    :param activation_fn: activation function to be applied after each convolutional layer
    :return: ResNet-18 model
    """
    inputs = tf.keras.Input(shape=(28, 28, 1))

    # Initial convolutional layer
    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding="same")(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(activation=activation_fn)(x)
    x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding="same")(x)

    # Residual blocks
    x = residual_block(inputs=x, out_channel=64, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=64, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=128, activation_fn=activation_fn, same_shape=True)
    x = residual_block(inputs=x, out_channel=128, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=256, activation_fn=activation_fn, same_shape=True)
    x = residual_block(inputs=x, out_channel=256, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=512, activation_fn=activation_fn, same_shape=True)
    x = residual_block(inputs=x, out_channel=512, activation_fn=activation_fn)

    # Global average pooling and dense layer
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(units=62, activation="softmax")(x)

    # Create and return the ResNet-18 model
    res_net = tf.keras.Model(inputs=inputs, outputs=x)

    return res_net


# Initialize
model_CNN = ResNet(activation_fn='relu')

model_CNN.summary()

"""Compile and train."""

# Instantiate optimizer and compile the model
model_CNN.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), loss="categorical_crossentropy",
                  metrics=['accuracy', 'Precision', 'Recall'])
model_CNN_results = model_CNN.fit(x=X_train_img_norm, y=y_train_one, batch_size=64, epochs=10, validation_split=0.2,
                                  shuffle=True)

"""Visualize the training results."""

# Convert the history result dictionary to a Pandas dataframe and extract the accuracies
loss_CNN = pd.DataFrame(model_CNN_results.history)[['loss', 'val_loss']]
accuracy_CNN = pd.DataFrame(model_CNN_results.history)[['accuracy', 'val_accuracy']]
precision_CNN = pd.DataFrame(model_CNN_results.history)[['precision', 'val_precision']]
recall_CNN = pd.DataFrame(model_CNN_results.history)[['recall', 'val_recall']]

# Plot
loss_CNN.plot()
plt.xlabel('Epoch')
plt.show()

accuracy_CNN.plot()
plt.xlabel('Epoch')
plt.show()

precision_CNN.plot()
plt.xlabel('Epoch')
plt.show()

recall_CNN.plot()
plt.xlabel('Epoch')
plt.show()

"""Calculate metrics on test set."""

# Evaluate the test data
loss_CNN_test, accuracy_CNN_test, precision_CNN_test, recall_CNN_test = model_CNN.evaluate(x=X_test_img_norm,
                                                                                           y=y_test_one)

pred_CNN = model_CNN.predict(x=X_test_img_norm)
cm_CNN = confusion_matrix(y_true=y_test, y_pred=np.argmax(pred_CNN, axis=1))

print("Loss on test set: {:.4f}".format(loss_CNN_test))
print("Accuracy on test set: {:.4f}%".format(100 * accuracy_CNN_test))
print("Precision on test set: {:.4f}%".format(100 * precision_CNN_test))
print("Recall on test set: {:.4f}%".format(100 * recall_CNN_test))
print("Confusion_matrix: \n", cm_CNN)

"""Classification report"""

print(classification_report(y_true=y_test, y_pred=pred_CNN))

"""Visualize the confusion matrix."""

plt.figure(figsize=(8, 6))
sns.heatmap(data=cm_CNN, fmt="d", cmap="RdBu_r", center=500)
plt.title("CNN Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

"""## 4. Hyperparameter Tuning

### 4.1 Hyperparameter Tuning for KNN

Grid search for KNN hyperparameter tuning.
"""

# grid-search with cross-validation for parameter tuning
param_grid_knn = {'n_neighbors': [1, 3, 5], 'weights': ['uniform', 'distance']}

grid_search_knn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid_knn, cv=2,
                               return_train_score=True, scoring=['accuracy', 'precision_macro', 'recall_macro'],
                               refit='accuracy')
grid_search_knn.fit(X=X_train_pca, y=y_train)

print("Best parameters: {}".format(grid_search_knn.best_params_))
print("Best cross-validation score: {:.2f}".format(grid_search_knn.best_score_))
print("Best estimator: {}".format(grid_search_knn.best_estimator_))
print("Test set score: {:.4f}".format(grid_search_knn.score(X_test_pca, y_test)))

"""Visualize the performance of different Hyperparameters."""

cv_results_knn = grid_search_knn.cv_results_

grid_accuracy_knn = []
grid_precision_knn = []
grid_recall_knn = []

# Append the metrics to the lists from the cv_results
for i in range(len(param_grid_knn['weights'])):
    temp1 = []
    temp2 = []
    temp3 = []
    for j in range(len(cv_results_knn['params'])):
        if param_grid_knn['weights'][i] == cv_results_knn['params'][j]['weights']:
            temp1.append(cv_results_knn['mean_test_accuracy'][j])
            temp2.append(cv_results_knn['mean_test_precision_macro'][j])
            temp3.append(cv_results_knn['mean_test_recall_macro'][j])
    grid_accuracy_knn.append(temp1)
    grid_precision_knn.append(temp2)
    grid_recall_knn.append(temp3)

# Plot
for i in range(len(param_grid_knn['weights'])):
    plt.plot(range(len(param_grid_knn['n_neighbors'])), grid_accuracy_knn[i],
             label=("weights={}".format(param_grid_knn['weights'][i])))
plt.legend()
plt.title("Accuracy")
plt.xticks(range(len(param_grid_knn['n_neighbors'])), param_grid_knn['n_neighbors'])
plt.xlabel("n_neighbors")
plt.show()

for i in range(len(param_grid_knn['weights'])):
    plt.plot(range(len(param_grid_knn['n_neighbors'])), grid_precision_knn[i],
             label=("weights={}".format(param_grid_knn['weights'][i])))
plt.legend()
plt.title("Precision")
plt.xticks(range(len(param_grid_knn['n_neighbors'])), param_grid_knn['n_neighbors'])
plt.xlabel("n_neighbors")
plt.show()

for i in range(len(param_grid_knn['weights'])):
    plt.plot(range(len(param_grid_knn['n_neighbors'])), grid_recall_knn[i],
             label=("weights={}".format(param_grid_knn['weights'][i])))
plt.legend()
plt.title("Recall")
plt.xticks(range(len(param_grid_knn['n_neighbors'])), param_grid_knn['n_neighbors'])
plt.xlabel("n_neighbors")
plt.show()

"""### 4.2 Hyperparameter Tuning for MLP

Grid search for MLP hyperparameter tuning.
"""

# Hyperparameter tuning initialisation
# define a function which allows us to quickly build a Keras model with our desired parameters
def build_mlp(n_hidden_layers=2, n_hidden_neurons=50, activation_function="relu", input_shape=[X_train_norm.shape[1]]):
    """
    Build a Keras MLP for 62 class classification with desired parameters.

    :param n_hidden_layers: number of hidden layers in the MLP
    :param n_hidden_neurons: number of neurons in each hidden layer
    :param activation_function: activation function to be used in the hidden layers
    :param input_shape: shape of the input data
    :return: constructed MLP model
    """
    # Model initialisation
    model = keras.models.Sequential()

    # Add the input layer
    model.add(keras.layers.Flatten(input_shape=input_shape))

    # Add the hidden layers with desired size and activation function
    for layer in range(n_hidden_layers):
        model.add(keras.layers.Dense(units=n_hidden_neurons, activation=activation_function))

    # Add the output layer for 100 class classification
    model.add(keras.layers.Dense(units=62, activation="softmax"))

    return model


# Create a KerasClassifier object which works with sklearn grid searches
# We need to pass default values of arguments in build_mlp if we wish to tune them
keras_classifier_NN = KerasClassifier(model=build_mlp, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),
                                      loss="categorical_crossentropy", batch_size=64,
                                      metrics=['accuracy', 'Precision', 'Recall'], epochs=10, shuffle=True,
                                      validation_split=0.2, n_hidden_neurons=50, activation_function="relu", )

# grid-search with cross-validation for parameter tuning
param_grid_NN = {"n_hidden_neurons": [100, 300, 500], "activation_function": ["relu", "sigmoid", "tanh"]}

grid_search_NN = GridSearchCV(estimator=keras_classifier_NN, param_grid=param_grid_NN, cv=2, return_train_score=True,
                              scoring=['accuracy', 'precision_macro', 'recall_macro'], refit='accuracy')
grid_search_NN.fit(X=X_train_norm, y=y_train_one)

print("Best parameters: {}".format(grid_search_NN.best_params_))
print("Best cross-validation score: {:.2f}".format(grid_search_NN.best_score_))
print("Best estimator: {}".format(grid_search_NN.best_estimator_))
print("Test set score: {:.4f}".format(grid_search_NN.score(X_test_norm, y_test_one)))

"""Visualize the performance of different Hyperparameters."""

cv_results_NN = grid_search_NN.cv_results_

grid_accuracy_NN = []
grid_precision_NN = []
grid_recall_NN = []

# Append the metrics to the lists from the cv_results
for i in range(len(param_grid_NN['activation_function'])):
    temp1 = []
    temp2 = []
    temp3 = []
    for j in range(len(cv_results_NN['params'])):
        if param_grid_NN['activation_function'][i] == cv_results_NN['params'][j]['activation_function']:
            temp1.append(cv_results_NN['mean_test_accuracy'][j])
            temp2.append(cv_results_NN['mean_test_precision_macro'][j])
            temp3.append(cv_results_NN['mean_test_recall_macro'][j])
    grid_accuracy_NN.append(temp1)
    grid_precision_NN.append(temp2)
    grid_recall_NN.append(temp3)

for i in range(len(param_grid_NN['activation_function'])):
    plt.plot(range(len(param_grid_NN['n_hidden_neurons'])), grid_accuracy_NN[i],
             label=("activation_function={}".format(param_grid_NN['activation_function'][i])))
plt.legend()
plt.title("Accuracy")
plt.xticks(range(len(param_grid_NN['n_hidden_neurons'])), param_grid_NN['n_hidden_neurons'])
plt.xlabel("n_hidden_neurons")
plt.show()

for i in range(len(param_grid_NN['activation_function'])):
    plt.plot(range(len(param_grid_NN['n_hidden_neurons'])), grid_precision_NN[i],
             label=("activation_function={}".format(param_grid_NN['activation_function'][i])))
plt.legend()
plt.title("Precision")
plt.xticks(range(len(param_grid_NN['n_hidden_neurons'])), param_grid_NN['n_hidden_neurons'])
plt.xlabel("n_hidden_neurons")
plt.show()

for i in range(len(param_grid_NN['activation_function'])):
    plt.plot(range(len(param_grid_NN['n_hidden_neurons'])), grid_recall_NN[i],
             label=("activation_function={}".format(param_grid_NN['activation_function'][i])))
plt.legend()
plt.title("Recall")
plt.xticks(range(len(param_grid_NN['n_hidden_neurons'])), param_grid_NN['n_hidden_neurons'])
plt.xlabel("n_hidden_neurons")
plt.show()

"""### 4.3 ResNet-18

Grid search for MLP hyperparameter tuning.
"""

# The residual block
def residual_block(inputs, out_channel, activation_fn, same_shape=False):
    """
    The residual block in ResNet-18.

    :param inputs: input tensor to the residual block
    :param out_channel: number of output channels/filters for the convolutional layers in the block
    :param activation_fn: activation function to be applied after each convolutional layer
    :param same_shape: a boolean flag indicating whether the input and output tensors should have the same shape
                       If False, the input tensor will be downsampled by a factor of 2
    :return: output tensor of the residual block
    """
    strides = (2, 2) if same_shape else (1, 1)

    # Convolution layers
    x = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=(3, 3), strides=strides, padding="same")(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(activation=activation_fn)(x)
    x = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=(3, 3), padding="same")(x)
    x = tf.keras.layers.BatchNormalization()(x)

    # Downsample the input if needed
    if same_shape:
        inputs = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=(1, 1), strides=(2, 2), padding="same")(inputs)

    # Add the residual connection
    x = tf.keras.layers.Add()([x, inputs])
    x = tf.keras.layers.Activation(activation=activation_fn)(x)

    return x


# ResNet 18
def ResNet(activation_fn='relu'):
    """
    ResNet-18 Model.

    :param activation_fn: activation function to be applied after each convolutional layer
    :return: ResNet-18 model
    """
    inputs = tf.keras.Input(shape=(28, 28, 1))

    # Initial convolutional layer
    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding="same")(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(activation=activation_fn)(x)
    x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding="same")(x)

    # Residual blocks
    x = residual_block(inputs=x, out_channel=64, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=64, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=128, activation_fn=activation_fn, same_shape=True)
    x = residual_block(inputs=x, out_channel=128, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=256, activation_fn=activation_fn, same_shape=True)
    x = residual_block(inputs=x, out_channel=256, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=512, activation_fn=activation_fn, same_shape=True)
    x = residual_block(inputs=x, out_channel=512, activation_fn=activation_fn)

    # Global average pooling and dense layer
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(units=62, activation="softmax")(x)

    # Create and return the ResNet-18 model
    res_net = tf.keras.Model(inputs=inputs, outputs=x)

    return res_net


# Create a KerasClassifier object which works with sklearn grid searches
# We need to pass default values of arguments in build_mlp if we wish to tune them
keras_classifier_CNN = KerasClassifier(model=ResNet, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),
                                       loss="categorical_crossentropy", batch_size=64,
                                       metrics=['accuracy', 'Precision', 'Recall'], epochs=10, shuffle=True,
                                       validation_split=0.2, activation_fn='relu')

# grid-search with cross-validation for parameter tuning
param_grid_CNN = {"batch_size": [32, 64, 128], "activation": ["relu", "sigmoid", "tanh"]}

grid_search_CNN = GridSearchCV(estimator=keras_classifier_CNN, param_grid=param_grid_CNN, cv=2, return_train_score=True,
                               scoring=['accuracy', 'precision_macro', 'recall_macro'], refit='accuracy')
grid_search_CNN.fit(X=X_train_img_norm, y=y_train_one)

print("Best parameters: {}".format(grid_search_CNN.best_params_))
print("Best cross-validation score: {:.2f}".format(grid_search_CNN.best_score_)) 
print("Best estimator: {}".format(grid_search_CNN.best_estimator_))
print("Test set score: {:.4f}".format(grid_search_CNN.score(X_test_img_norm, y_test_one)))

"""Visualize the performance of different Hyperparameters."""

cv_results_CNN = grid_search_CNN.cv_results_

grid_accuracy_CNN = []
grid_precision_CNN = []
grid_recall_CNN = []

# Append the metrics to the lists from the cv_results
for i in range(len(param_grid_CNN['activation_fn'])):
    temp1 = []
    temp2 = []
    temp3 = []
    for j in range(len(cv_results_CNN['params'])):
        if param_grid_CNN['activation_fn'][i] == cv_results_CNN['params'][j]['activation_fn']:
            temp1.append(cv_results_CNN['mean_test_accuracy'][j])
            temp2.append(cv_results_CNN['mean_test_precision_macro'][j])
            temp3.append(cv_results_CNN['mean_test_recall_macro'][j])
    grid_accuracy_CNN.append(temp1)
    grid_precision_CNN.append(temp2)
    grid_recall_CNN.append(temp3)

for i in range(len(param_grid_CNN['activation_fn'])):
    plt.plot(range(len(param_grid_CNN['batch_size'])), grid_accuracy_CNN[i],
             label=("activation_fn={}".format(param_grid_CNN['activation_fn'][i])))
plt.legend()
plt.title("Accuracy")
plt.xticks(range(len(param_grid_CNN['batch_size'])), param_grid_CNN['batch_size'])
plt.xlabel("batch_size")
plt.show()

for i in range(len(param_grid_CNN['activation_fn'])):
    plt.plot(range(len(param_grid_CNN['batch_size'])), grid_precision_CNN[i],
             label=("activation_fn={}".format(param_grid_CNN['activation_fn'][i])))
plt.legend()
plt.title("Precision")
plt.xticks(range(len(param_grid_CNN['batch_size'])), param_grid_CNN['batch_size'])
plt.xlabel("batch_size")
plt.show()

for i in range(len(param_grid_CNN['activation_fn'])):
    plt.plot(range(len(param_grid_CNN['batch_size'])), grid_recall_CNN[i],
             label=("activation_fn={}".format(param_grid_CNN['activation_fn'][i])))
plt.legend()
plt.title("Recall")
plt.xticks(range(len(param_grid_CNN['batch_size'])), param_grid_CNN['batch_size'])
plt.xlabel("batch_size")
plt.show()

"""## 5. Final Models & Model Evaluation

### 5.1 Best KNN
"""

# Final model for kNN
best_knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
best_knn.fit(X=X_train_pca, y=y_train)

best_knn_accuracy = best_knn.score(X=X_test_pca, y=y_test)

best_knn_pred = best_knn.predict(X=X_test_pca)

best_knn_precision = precision_score(y_true=y_test, y_pred=best_knn_pred, average='macro')
best_knn_recall = recall_score(y_true=y_test, y_pred=best_knn_pred, average='macro')
best_knn_cm = confusion_matrix(y_true=y_test, y_pred=best_knn_pred)

print("Accuracy on test set: {:.3f}%".format(100 * best_knn_accuracy))
print("Precision on test set: {:.3f}%".format(100 * best_knn_precision))
print("Recall on test set: {:.3f}%".format(100 * best_knn_recall))
print("Confusion_matrix: \n", best_knn_cm)

print(classification_report(y_true=y_test, y_pred=best_knn_pred))

plt.figure(figsize=(8, 6))
sns.heatmap(data=best_knn_cm, fmt="d", cmap="RdBu_r", center=500)
plt.title("knn Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

"""### 5.2 Best MLP"""

# Model Initialisation
best_NN = keras.Sequential()
# Input layer
best_NN.add(keras.layers.Flatten(input_shape=[X_train_norm.shape[1]]))
# Hidden Layer 1
best_NN.add(keras.layers.Dense(units=500, activation='sigmoid'))
# Hidden Layer 2
best_NN.add(keras.layers.Dense(units=500, activation='sigmoid'))
# Output Layer
best_NN.add(keras.layers.Dense(units=62, activation='softmax'))

best_NN.summary()

# Instantiate optimiser and compile the model.
best_NN.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),
                metrics=['accuracy', 'Precision', 'Recall'])

# Train the classifier
best_NN_results = best_NN.fit(x=X_train_norm, y=y_train_one, epochs=10, batch_size=64, shuffle=True,
                              validation_split=0.2)

# Convert the history result dictionary to a Pandas dataframe and extract the accuracies
best_NN_loss = pd.DataFrame(best_NN_results.history)[['loss', 'val_loss']]
best_NN_accuracy = pd.DataFrame(best_NN_results.history)[['accuracy', 'val_accuracy']]
best_NN_precision = pd.DataFrame(best_NN_results.history)[['precision', 'val_precision']]
best_NN_recall = pd.DataFrame(best_NN_results.history)[['recall', 'val_recall']]

# Plot
best_NN_loss.plot()
plt.xlabel('Epoch')
plt.show()

best_NN_accuracy.plot()
plt.xlabel('Epoch')
plt.show()

best_NN_precision.plot()
plt.xlabel('Epoch')
plt.show()

best_NN_recall.plot()
plt.xlabel('Epoch')
plt.show()

best_NN_loss_test, best_NN_accuracy_test, best_NN_precision_test, best_NN_recall_test = model_NN.evaluate(x=X_test_norm,
                                                                                                          y=y_test_one)

best_NN_pred = best_NN.predict(x=X_test_norm)
best_NN_cm = confusion_matrix(y_true=y_test, y_pred=np.argmax(best_NN_pred, axis=1))

print("Loss on test set: {:.4f}".format(best_NN_loss_test))
print("Accuracy on test set: {:.4f}%".format(100 * best_NN_accuracy_test))
print("Precision on test set: {:.4f}%".format(100 * best_NN_precision_test))
print("Recall on test set: {:.4f}%".format(100 * best_NN_recall_test))
print("Confusion_matrix: \n", best_NN_cm)

print(classification_report(y_true=y_test, y_pred=np.argmax(best_NN_pred, axis=1)))

plt.figure(figsize=(8, 6))
sns.heatmap(data=best_NN_cm, fmt="d", cmap="RdBu_r", center=500)
plt.title("NN Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

"""### 5.3 Best ResNet-18"""

# The residual block
def residual_block(inputs, out_channel, activation_fn, same_shape=False):
    """
    The residual block in ResNet-18.

    :param inputs: input tensor to the residual block
    :param out_channel: number of output channels/filters for the convolutional layers in the block
    :param activation_fn: activation function to be applied after each convolutional layer
    :param same_shape: a boolean flag indicating whether the input and output tensors should have the same shape
                       If False, the input tensor will be downsampled by a factor of 2
    :return: output tensor of the residual block
    """
    strides = (2, 2) if same_shape else (1, 1)

    # Convolution layers
    x = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=(3, 3), strides=strides, padding="same")(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(activation=activation_fn)(x)

    x = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=(3, 3), padding="same")(x)
    x = tf.keras.layers.BatchNormalization()(x)

    # Downsample the input if needed
    if same_shape:
        inputs = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=(1, 1), strides=(2, 2), padding="same")(inputs)

    # Add the residual connection
    x = tf.keras.layers.Add()([x, inputs])
    x = tf.keras.layers.Activation(activation=activation_fn)(x)

    return x


# ResNet 18
def ResNet(activation_fn):
    """
    ResNet-18 Model.

    :param activation_fn: activation function to be applied after each convolutional layer
    :return: ResNet-18 model
    """
    inputs = tf.keras.Input(shape=(28, 28, 1))

    # Initial convolutional layer
    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding="same")(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(activation=activation_fn)(x)
    x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding="same")(x)

    # Residual blocks
    x = residual_block(inputs=x, out_channel=64, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=64, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=128, activation_fn=activation_fn, same_shape=True)
    x = residual_block(inputs=x, out_channel=128, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=256, activation_fn=activation_fn, same_shape=True)
    x = residual_block(inputs=x, out_channel=256, activation_fn=activation_fn)
    x = residual_block(inputs=x, out_channel=512, activation_fn=activation_fn, same_shape=True)
    x = residual_block(inputs=x, out_channel=512, activation_fn=activation_fn)

    # Global average pooling and dense layer
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(units=62, activation="softmax")(x)

    # Create and return the ResNet-18 model
    res_net = tf.keras.Model(inputs=inputs, outputs=x)

    return res_net


# Initialize
best_CNN = ResNet(activation_fn='relu')

best_CNN.summary()

best_CNN.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), loss="categorical_crossentropy",
                 metrics=['accuracy', 'Precision', 'Recall'])
best_CNN_results = best_CNN.fit(x=X_train_img_norm, y=y_train_one, batch_size=64, epochs=10, validation_split=0.2,
                                shuffle=True)

# Convert the history result dictionary to a Pandas dataframe and extract the accuracies
best_CNN_loss = pd.DataFrame(best_CNN_results.history)[['loss', 'val_loss']]
best_CNN_accuracy = pd.DataFrame(best_CNN_results.history)[['accuracy', 'val_accuracy']]
best_CNN_precision = pd.DataFrame(best_CNN_results.history)[['precision', 'val_precision']]
best_CNN_recall = pd.DataFrame(best_CNN_results.history)[['recall', 'val_recall']]

# Plot
best_CNN_loss.plot()
plt.xlabel('Epoch')
plt.show()

best_CNN_accuracy.plot()
plt.xlabel('Epoch')
plt.show()

best_CNN_precision.plot()
plt.xlabel('Epoch')
plt.show()

best_CNN_recall.plot()
plt.xlabel('Epoch')
plt.show()

best_CNN_loss_test, best_CNN_accuracy_test, best_CNN_precision_test, best_CNN_recall_test = model_CNN.evaluate(
    x=X_test_img_norm, y=y_test_one)

best_CNN_pred = model_CNN.predict(x=X_test_img_norm)
best_CNN_cm = confusion_matrix(y_true=y_test, y_pred=np.argmax(best_CNN_pred, axis=1))

print("Loss on test set: {:.4f}".format(best_CNN_loss_test))
print("Accuracy on test set: {:.4f}%".format(100 * best_CNN_accuracy_test))
print("Precision on test set: {:.4f}%".format(100 * best_CNN_precision_test))
print("Recall on test set: {:.4f}%".format(100 * best_CNN_recall_test))
print("Confusion_matrix: \n", best_CNN_cm)

classification_report(y_true=y_test, y_pred=np.argmax(best_CNN_pred, axis=1))Z

plt.figure(figsize=(8, 6))
sns.heatmap(data=best_CNN_cm, fmt="d", cmap="RdBu_r", center=500)
plt.title("CNN Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()